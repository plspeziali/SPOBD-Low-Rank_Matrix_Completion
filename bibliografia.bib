@article{lowrank,

  author={Chi, Yuejie},

  journal={IEEE Signal Processing Magazine}, 

  title={Low-Rank Matrix Completion [Lecture Notes]}, 

  year={2018},

  volume={35},

  number={5},

  pages={178-181},

  abstract={Imagine one observes a small subset of entries in a large matrix and aims to recover the entire matrix. Without a priori knowledge of the matrix, this problem is highly ill-posed. Fortunately, data matrices often exhibit low-dimensional structures that can be used effectively to regularize the solution space. The celebrated effectiveness of principal component analysis (PCA) in science and engineering suggests that most variability of real-world data can be accounted for by projecting the data onto a few directions known as the principal components. Correspondingly, the data matrix can be modeled as a low-rank matrix, at least approximately. Is it possible to complete a partially observed matrix if its rank, i.e., its maximum number of linearly independent row or column vectors, is small?},

  keywords={},

  doi={10.1109/MSP.2018.2832197},

  ISSN={1558-0792},

  month={Sep.},
  }
